{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T10:27:10.396997Z",
     "iopub.status.busy": "2025-03-14T10:27:10.396721Z",
     "iopub.status.idle": "2025-03-14T10:27:37.130209Z",
     "shell.execute_reply": "2025-03-14T10:27:37.129493Z",
     "shell.execute_reply.started": "2025-03-14T10:27:10.396965Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.1)\n",
      "Collecting TorchCRF\n",
      "  Downloading TorchCRF-1.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.12)\n",
      "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from TorchCRF) (2.5.1+cu121)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->TorchCRF) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->TorchCRF) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->TorchCRF) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.0.0->TorchCRF) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->TorchCRF) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Downloading TorchCRF-1.1.0-py3-none-any.whl (5.2 kB)\n",
      "Installing collected packages: TorchCRF\n",
      "Successfully installed TorchCRF-1.1.0\n"
     ]
    }
   ],
   "source": [
    "#############################\n",
    "# Install and Import Packages\n",
    "#############################\n",
    "%pip install transformers datasets TorchCRF  # Install required packages\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set WandB API key (if you wish to use wandb logging; otherwise, set report_to=\"none\")\n",
    "os.environ[\"WANDB_API_KEY\"] = \"9005f3d03485025996bc83adb773e446b2887451\"\n",
    "\n",
    "# Import CRF from TorchCRF (this version does NOT support the 'batch_first' argument)\n",
    "from TorchCRF import CRF\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    AutoModel,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T10:27:37.131593Z",
     "iopub.status.busy": "2025-03-14T10:27:37.131012Z",
     "iopub.status.idle": "2025-03-14T10:27:41.468534Z",
     "shell.execute_reply": "2025-03-14T10:27:41.467775Z",
     "shell.execute_reply.started": "2025-03-14T10:27:37.131566Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded local SQuAD v2 dataset:\n",
      "DatasetDict({\n",
      "    train_full: Dataset({\n",
      "        features: ['context', 'question', 'answers'],\n",
      "        num_rows: 130319\n",
      "    })\n",
      "    test_official: Dataset({\n",
      "        features: ['context', 'question', 'answers'],\n",
      "        num_rows: 11873\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#############################\n",
    "# 1. Load Local SQuAD v2 Data\n",
    "#############################\n",
    "\n",
    "train_path = \"/kaggle/input/squad-v2/transformers/default/1/train-v2.0.json\"\n",
    "dev_path   = \"/kaggle/input/squad-v2/transformers/default/1/dev-v2.0.json\"\n",
    "\n",
    "def load_squad_v2(json_path):\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        squad_dict = json.load(f)\n",
    "\n",
    "    flattened = []\n",
    "    for article in squad_dict[\"data\"]:\n",
    "        for paragraph in article[\"paragraphs\"]:\n",
    "            context = paragraph[\"context\"]\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                question = qa[\"question\"]\n",
    "                if \"answers\" in qa and len(qa[\"answers\"]) > 0:\n",
    "                    answers = {\n",
    "                        \"text\": [ans[\"text\"] for ans in qa[\"answers\"]],\n",
    "                        \"answer_start\": [ans[\"answer_start\"] for ans in qa[\"answers\"]]\n",
    "                    }\n",
    "                else:\n",
    "                    answers = {\"text\": [], \"answer_start\": []}\n",
    "                \n",
    "                flattened.append({\n",
    "                    \"context\": context,\n",
    "                    \"question\": question,\n",
    "                    \"answers\": answers\n",
    "                })\n",
    "    return flattened\n",
    "\n",
    "train_data = load_squad_v2(train_path)\n",
    "dev_data   = load_squad_v2(dev_path)\n",
    "\n",
    "# Convert to Hugging Face Datasets\n",
    "train_dataset_full = Dataset.from_list(train_data)\n",
    "official_dev_dataset = Dataset.from_list(dev_data)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train_full\": train_dataset_full,\n",
    "    \"test_official\": official_dev_dataset\n",
    "})\n",
    "\n",
    "print(\"Loaded local SQuAD v2 dataset:\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T10:27:41.470423Z",
     "iopub.status.busy": "2025-03-14T10:27:41.470196Z",
     "iopub.status.idle": "2025-03-14T10:27:44.277967Z",
     "shell.execute_reply": "2025-03-14T10:27:44.277099Z",
     "shell.execute_reply.started": "2025-03-14T10:27:41.470403Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22680e474b384b83bfc0ef2ab155a65e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/413 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9be8664d863340f595ca93c8874154a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#############################\n",
    "# 2. 80:20 Split & Basic Setup\n",
    "#############################\n",
    "\n",
    "seed_val = 42\n",
    "# Subsample 15k examples from the full training set (for speed)\n",
    "train_dataset_full = train_dataset_full.shuffle(seed=seed_val).select(range(15000))\n",
    "\n",
    "# Split 80:20 into training and validation sets\n",
    "split_dataset = train_dataset_full.train_test_split(test_size=0.2, seed=seed_val)\n",
    "train_dataset = split_dataset[\"train\"]   # ~80%\n",
    "val_dataset   = split_dataset[\"test\"]    # ~20%\n",
    "\n",
    "# Reserve the official dev set for final evaluation\n",
    "test_dataset  = dataset[\"test_official\"]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "\n",
    "#############################\n",
    "# 3. Tokenizer\n",
    "#############################\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"SpanBERT/spanbert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T10:27:44.279625Z",
     "iopub.status.busy": "2025-03-14T10:27:44.279383Z",
     "iopub.status.idle": "2025-03-14T10:27:58.747150Z",
     "shell.execute_reply": "2025-03-14T10:27:58.746474Z",
     "shell.execute_reply.started": "2025-03-14T10:27:44.279605Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e6386bfca1f4aa39520d230cf811f91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "435ef3618ee94d7b8ff917e397e8bad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99eaa2c02ec847d59dd5644617b222e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11873 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#############################\n",
    "# 4. Preprocessing for Baseline QA\n",
    "#############################\n",
    "\n",
    "def preprocess_qa_examples(examples):\n",
    "    inputs = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=384,\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    \n",
    "    sample_mapping = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    \n",
    "    start_positions = []\n",
    "    end_positions   = []\n",
    "    \n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "        \n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            answer_start = answers[\"answer_start\"][0]\n",
    "            answer_text  = answers[\"text\"][0]\n",
    "            answer_end   = answer_start + len(answer_text)\n",
    "            \n",
    "            token_start_index = 0\n",
    "            token_end_index   = len(offsets) - 1\n",
    "            \n",
    "            while token_start_index < len(offsets) and offsets[token_start_index][0] <= 0:\n",
    "                token_start_index += 1\n",
    "            while token_end_index >= 0 and offsets[token_end_index][1] == 0:\n",
    "                token_end_index -= 1\n",
    "            \n",
    "            start_index, end_index = token_start_index, token_end_index\n",
    "            while start_index < len(offsets) and offsets[start_index][0] < answer_start:\n",
    "                start_index += 1\n",
    "            while end_index >= 0 and offsets[end_index][1] > answer_end:\n",
    "                end_index -= 1\n",
    "            \n",
    "            if start_index >= len(offsets) or end_index < 0 or start_index > end_index:\n",
    "                start_positions.append(0)\n",
    "                end_positions.append(0)\n",
    "            else:\n",
    "                start_positions.append(start_index)\n",
    "                end_positions.append(end_index)\n",
    "    \n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"]   = end_positions\n",
    "    return inputs\n",
    "\n",
    "# Preprocess the baseline datasets\n",
    "tokenized_train = train_dataset.map(preprocess_qa_examples, batched=True, remove_columns=train_dataset.column_names)\n",
    "tokenized_val   = val_dataset.map(preprocess_qa_examples,   batched=True, remove_columns=val_dataset.column_names)\n",
    "tokenized_test  = test_dataset.map(preprocess_qa_examples,  batched=True, remove_columns=test_dataset.column_names)\n",
    "\n",
    "#############################\n",
    "# 5. Exact Match Metric\n",
    "#############################\n",
    "\n",
    "def exact_match_score(predictions, references):\n",
    "    assert len(predictions) == len(references), \"Lists must have the same length\"\n",
    "    matches = sum(p == r for p, r in zip(predictions, references))\n",
    "    return matches / len(references) * 100  # percentage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T10:27:58.748162Z",
     "iopub.status.busy": "2025-03-14T10:27:58.747900Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bbcdde2177f42afbdebaf0097a143b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/215M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at SpanBERT/spanbert-base-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "<ipython-input-5-7957e7d06cc1>:26: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  baseline_trainer = Trainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training baseline SpanBERT model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mamartya22062\u001b[0m (\u001b[33mamartya22062-indraprastha-institute-of-information-techn\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250314_102811-3ack7e6w</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/amartya22062-indraprastha-institute-of-information-techn/huggingface/runs/3ack7e6w' target=\"_blank\">./results_baseline</a></strong> to <a href='https://wandb.ai/amartya22062-indraprastha-institute-of-information-techn/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/amartya22062-indraprastha-institute-of-information-techn/huggingface' target=\"_blank\">https://wandb.ai/amartya22062-indraprastha-institute-of-information-techn/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/amartya22062-indraprastha-institute-of-information-techn/huggingface/runs/3ack7e6w' target=\"_blank\">https://wandb.ai/amartya22062-indraprastha-institute-of-information-techn/huggingface/runs/3ack7e6w</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9098' max='9180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9098/9180 54:08 < 00:29, 2.80 it/s, Epoch 5.95/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.005400</td>\n",
       "      <td>1.797424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.851800</td>\n",
       "      <td>2.357016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.596700</td>\n",
       "      <td>1.871988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.093900</td>\n",
       "      <td>2.090552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.751200</td>\n",
       "      <td>2.169038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#############################\n",
    "# 6. Baseline Model: SpanBERT\n",
    "#############################\n",
    "\n",
    "baseline_model = AutoModelForQuestionAnswering.from_pretrained(\"SpanBERT/spanbert-base-cased\")\n",
    "baseline_model.to(device)\n",
    "\n",
    "baseline_training_args = TrainingArguments(\n",
    "    output_dir=\"./results_baseline\",\n",
    "    eval_strategy=\"epoch\",        # Use eval_strategy instead of evaluation_strategy\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=6,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100,\n",
    "    save_strategy=\"epoch\",\n",
    "    push_to_hub=False,\n",
    "    report_to=\"wandb\"  # Use \"none\" to disable WandB logging\n",
    ")\n",
    "\n",
    "# (Optional) A dummy compute_metrics function so that Trainer prints evaluation metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    return {}\n",
    "\n",
    "baseline_trainer = Trainer(\n",
    "    model=baseline_model,\n",
    "    args=baseline_training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"Training baseline SpanBERT model...\")\n",
    "baseline_train_result = baseline_trainer.train()\n",
    "baseline_trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_crf_labels(offset_mapping, answer_start, answer_text):\n",
    "    labels = [0] * len(offset_mapping)  # 0=O, 1=B, 2=I\n",
    "    if answer_text == \"\":\n",
    "        return labels\n",
    "    answer_end = answer_start + len(answer_text)\n",
    "    found_b = False\n",
    "    for i, (start, end) in enumerate(offset_mapping):\n",
    "        if start is None or end is None or (start == 0 and end == 0):\n",
    "            continue\n",
    "        if start >= answer_start and end <= answer_end:\n",
    "            if not found_b:\n",
    "                labels[i] = 1  # B\n",
    "                found_b = True\n",
    "            else:\n",
    "                labels[i] = 2  # I\n",
    "    return labels\n",
    "\n",
    "def preprocess_crf_examples(examples):\n",
    "    inputs = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=384,\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    sample_mapping = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    \n",
    "    all_labels = []\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            label_ids = [0] * len(offsets)\n",
    "        else:\n",
    "            answer_start = answers[\"answer_start\"][0]\n",
    "            answer_text  = answers[\"text\"][0]\n",
    "            label_ids = create_crf_labels(offsets, answer_start, answer_text)\n",
    "        all_labels.append(label_ids)\n",
    "    \n",
    "    inputs[\"labels\"] = all_labels\n",
    "    return inputs\n",
    "\n",
    "# Preprocess for CRF model\n",
    "tokenized_train_crf = train_dataset.map(preprocess_crf_examples, batched=True, remove_columns=train_dataset.column_names)\n",
    "tokenized_val_crf   = val_dataset.map(preprocess_crf_examples,   batched=True, remove_columns=val_dataset.column_names)\n",
    "tokenized_test_crf  = test_dataset.map(preprocess_crf_examples,  batched=True, remove_columns=test_dataset.column_names)\n",
    "\n",
    "# Ensure datasets return PyTorch tensors\n",
    "tokenized_train_crf.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "tokenized_val_crf.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "tokenized_test_crf.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "class SpanBERTCRFForQA(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(SpanBERTCRFForQA, self).__init__()\n",
    "        self.spanbert = AutoModel.from_pretrained(model_name)\n",
    "        # Increase dropout to mitigate overfitting\n",
    "        self.dropout  = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(self.spanbert.config.hidden_size, 3)\n",
    "        # Initialize CRF; our TorchCRF expects input in shape (batch, seq_len, num_tags)\n",
    "        self.crf = CRF(3)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.spanbert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = self.dropout(outputs.last_hidden_state)  # (batch, seq_len, hidden_size)\n",
    "        emissions = self.classifier(sequence_output)               # (batch, seq_len, 3)\n",
    "        # Do NOT transpose: keep emissions as (batch, seq_len, num_tags)\n",
    "        mask = attention_mask.bool()  # (batch, seq_len)\n",
    "        \n",
    "        if labels is not None:\n",
    "            labels = labels.long()\n",
    "            loss = -self.crf(emissions, labels, mask=mask)\n",
    "            return {\"loss\": loss, \"emissions\": emissions}\n",
    "        else:\n",
    "            pred_tags = self.crf.decode(emissions, mask=mask)\n",
    "            return pred_tags\n",
    "\n",
    "crf_model = SpanBERTCRFForQA(\"SpanBERT/spanbert-base-cased\")\n",
    "crf_model.to(device)\n",
    "\n",
    "# Define a custom Trainer to override compute_loss.\n",
    "from transformers import Trainer\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, num_items_in_batch=None, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs[\"loss\"]\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "crf_training_args = TrainingArguments(\n",
    "    output_dir=\"./results_crf\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=6,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100,\n",
    "    save_strategy=\"epoch\",\n",
    "    push_to_hub=False,\n",
    "    report_to=\"wandb\"  # Set to \"none\" to disable wandb logging if desired\n",
    ")\n",
    "\n",
    "# Use CustomTrainer for CRF model training.\n",
    "crf_trainer = CustomTrainer(\n",
    "    model=crf_model,\n",
    "    args=crf_training_args,\n",
    "    train_dataset=tokenized_train_crf,\n",
    "    eval_dataset=tokenized_val_crf,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(\"Training SpanBERT-CRF model...\")\n",
    "crf_train_result = crf_trainer.train()\n",
    "crf_trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#############################\n",
    "# 8. Plot Training/Validation Loss\n",
    "#############################\n",
    "\n",
    "def plot_trainer_logs(trainer, title):\n",
    "    logs = trainer.state.log_history\n",
    "    train_loss = []\n",
    "    eval_loss  = []\n",
    "    epochs_tl  = []\n",
    "    epochs_el  = []\n",
    "    for log in logs:\n",
    "        if \"loss\" in log and \"epoch\" in log:\n",
    "            train_loss.append(log[\"loss\"])\n",
    "            epochs_tl.append(log[\"epoch\"])\n",
    "        if \"eval_loss\" in log and \"epoch\" in log:\n",
    "            eval_loss.append(log[\"eval_loss\"])\n",
    "            epochs_el.append(log[\"epoch\"])\n",
    "    \n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.plot(epochs_tl, train_loss, label=\"Train Loss\")\n",
    "    plt.plot(epochs_el, eval_loss, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Plotting Baseline (SpanBERT) Loss Curves...\")\n",
    "plot_trainer_logs(baseline_trainer, \"Baseline SpanBERT QA\")\n",
    "\n",
    "print(\"Plotting CRF Model (SpanBERT-CRF) Loss Curves...\")\n",
    "plot_trainer_logs(crf_trainer, \"SpanBERT-CRF QA\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#############################\n",
    "# 9. Evaluation: Exact Match Score\n",
    "#############################\n",
    "\n",
    "def postprocess_qa_predictions(features, raw_predictions):\n",
    "    start_logits, end_logits = raw_predictions\n",
    "    predictions = []\n",
    "    for i in range(len(features[\"input_ids\"])):\n",
    "        input_ids = features[\"input_ids\"][i]\n",
    "        start_idx = int(np.argmax(start_logits[i]))\n",
    "        end_idx   = int(np.argmax(end_logits[i]))\n",
    "        if start_idx > end_idx:\n",
    "            predictions.append(\"\")\n",
    "        else:\n",
    "            pred_ids = input_ids[start_idx : end_idx+1]\n",
    "            prediction = tokenizer.decode(\n",
    "                pred_ids,\n",
    "                skip_special_tokens=True,\n",
    "                clean_up_tokenization_spaces=True\n",
    "            )\n",
    "            predictions.append(prediction)\n",
    "    return predictions\n",
    "\n",
    "# Evaluate on the official dev set (test_dataset)\n",
    "tokenized_test_baseline = tokenized_test\n",
    "baseline_raw_preds = baseline_trainer.predict(tokenized_test_baseline)\n",
    "baseline_preds = postprocess_qa_predictions(\n",
    "    tokenized_test_baseline,\n",
    "    (baseline_raw_preds.predictions[0], baseline_raw_preds.predictions[1])\n",
    ")\n",
    "\n",
    "baseline_refs = [\n",
    "    ex[\"answers\"][\"text\"][0] if len(ex[\"answers\"][\"text\"]) > 0 else \"\"\n",
    "    for ex in test_dataset\n",
    "]\n",
    "baseline_em = exact_match_score(baseline_preds, baseline_refs)\n",
    "print(f\"\\nBaseline SpanBERT Exact Match (Official Dev): {baseline_em:.2f}%\")\n",
    "\n",
    "# CRF Model Evaluation on official dev set\n",
    "tokenized_test_crf_final = tokenized_test_crf\n",
    "crf_model.eval()\n",
    "crf_predictions = []\n",
    "for i in range(len(tokenized_test_crf_final)):\n",
    "    input_ids      = torch.tensor(tokenized_test_crf_final[i][\"input_ids\"]).unsqueeze(0).to(device)\n",
    "    attention_mask = torch.tensor(tokenized_test_crf_final[i][\"attention_mask\"]).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred_tags = crf_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    # Reconstruct answer text from the predicted tags\n",
    "    answer_tokens = []\n",
    "    found_b = False\n",
    "    for tag, tid in zip(pred_tags[0], tokenized_test_crf_final[i][\"input_ids\"]):\n",
    "        if tag == 1:  # B tag\n",
    "            found_b = True\n",
    "            answer_tokens.append(tid)\n",
    "        elif found_b and tag == 2:  # I tag\n",
    "            answer_tokens.append(tid)\n",
    "        elif found_b:\n",
    "            break\n",
    "    if answer_tokens:\n",
    "        answer_text = tokenizer.decode(\n",
    "            answer_tokens,\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=True\n",
    "        )\n",
    "    else:\n",
    "        answer_text = \"\"\n",
    "    crf_predictions.append(answer_text)\n",
    "\n",
    "crf_refs = [\n",
    "    ex[\"answers\"][\"text\"][0] if len(ex[\"answers\"][\"text\"]) > 0 else \"\"\n",
    "    for ex in test_dataset\n",
    "]\n",
    "crf_em = exact_match_score(crf_predictions, crf_refs)\n",
    "print(f\"SpanBERT-CRF Exact Match (Official Dev): {crf_em:.2f}%\")\n",
    "\n",
    "#############################\n",
    "# 10. Final Notes\n",
    "#############################\n",
    "# 1) The training set was subsampled to 15k examples and split 80:20 for training vs. validation.\n",
    "# 2) The official dev set is used only for final evaluation.\n",
    "# 3) Loss curves are plotted locally and also logged to WandB if enabled.\n",
    "# 4) The CRF model now transposes emissions and mask to match TorchCRF's expected input shape.\n",
    "# 5) \"evaluation_strategy\" was replaced with \"eval_strategy\" to avoid deprecation warnings.\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "isSourceIdPinned": true,
     "modelId": 264818,
     "modelInstanceId": 243210,
     "sourceId": 283835,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
